{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facemesh base import & variables\n",
    "import numpy as np\n",
    "from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "\n",
    "# path, thres, thres_ear 설정\n",
    "thres = 0.45 # thres < 0.5 (select in 0.40 ~ 0.47)  => e.g. [thres|----|(1-thres)]  \n",
    "thres_ = 1-thres\n",
    "thres_ear = 0.7 # thres_ear >= 0.5 => up\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "FACEMESH_LEFT_EYE = frozenset([(263, 249), (249, 390), (390, 373), (373, 374),\n",
    "                               (374, 380), (380, 381), (381, 382), (382, 362),\n",
    "                               (263, 466), (466, 388), (388, 387), (387, 386),\n",
    "                               (386, 385), (385, 384), (384, 398), (398, 362)])\n",
    "FACEMESH_RIGHT_EYE = frozenset([(33, 7), (7, 163), (163, 144), (144, 145),\n",
    "                                (145, 153), (153, 154), (154, 155), (155, 133),\n",
    "                                (33, 246), (246, 161), (161, 160), (160, 159),\n",
    "                                (159, 158), (158, 157), (157, 173), (173, 133)])\n",
    "FACEMESH_CONTOURS = frozenset().union(*[FACEMESH_LEFT_EYE, FACEMESH_RIGHT_EYE])\n",
    "FACEMESH_RIGHT_IRIS = frozenset([(469, 470), (470, 471), (471, 472),(472, 469)])\n",
    "FACEMESH_LEFT_IRIS = frozenset([(474, 475), (475, 476), (476, 477),(477, 474)])\n",
    "FACEMESH_IRISES = frozenset().union(*[FACEMESH_LEFT_IRIS, FACEMESH_RIGHT_IRIS])\n",
    "FACEMESH_LEFT_EYE = frozenset([(263, 249), (249, 390), (390, 373), (373, 374),\n",
    "                       (374, 380), (380, 381), (381, 382), (382, 362),\n",
    "                       (263, 466), (466, 388), (388, 387), (387, 386),\n",
    "                       (386, 385), (385, 384), (384, 398), (398, 362)])\n",
    "FACEMESH_RIGHT_EYE = frozenset([(33, 7), (7, 163), (163, 144), (144, 145),\n",
    "                                (145, 153), (153, 154), (154, 155), (155, 133),\n",
    "                                (33, 246), (246, 161), (161, 160), (160, 159),\n",
    "                                (159, 158), (158, 157), (157, 173), (173, 133)])\n",
    "FACEMESH_EYES = frozenset().union(*[FACEMESH_LEFT_EYE, FACEMESH_RIGHT_EYE])\n",
    "right_under = [7,33,133,144,145,153,154,155,163]\n",
    "left_under = [249,263,362,373,374,380,381,382,390]\n",
    "under = frozenset().union(right_under,left_under) # under에는 양끝 눈꺼풀도 포함되어 있음\n",
    "# For webcam input:\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# variable\n",
    "ok_flag = 1\n",
    "total_landmarks=[]\n",
    "dir_r = None\n",
    "dir_l = None\n",
    "dir_ = None    \n",
    "aaaa=1\n",
    "time_list = []\n",
    "\n",
    "\n",
    "def distance(x1, y1, x2, y2):\n",
    "    result = math.sqrt( math.pow(x1 - x2, 2) + math.pow(y1 - y2, 2))\n",
    "    return result\n",
    "\n",
    "# yolov7 base import & variables\n",
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "import easydict\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "opt = easydict.EasyDict({\n",
    "    \"weights\":'yolov7-e6.pt',\n",
    "    \"source\":'0',\n",
    "    \n",
    "    \"img_size\":640,\n",
    "    \"conf_thres\":0.25,\n",
    "    \"iou_thres\":0.45,\n",
    "    \"device\":'',\n",
    "    \"view_img\":False,\n",
    "    \"save_txt\":False,\n",
    "    \"save_conf\":False,\n",
    "    \"nosave\":False,\n",
    "    \"classes\":None,\n",
    "    \n",
    "    \"agnostic_nms\":False,\n",
    "    \"augment\":False,\n",
    "    \"updata\":False,\n",
    "    \"project\":'runs/detect',\n",
    "    \"name\":'exp',\n",
    "    \"exist_ok\":False,\n",
    "    \"no_trace\":False,\n",
    "})\n",
    "# box1 = box1_x1,box1_y1,box1_x2,box1_y2\n",
    "# box2 = xyxy_\n",
    "def IoU(box1, box2): # box1이 그리드, box2가 객체 박스\n",
    "    global box1_x1,box1_y1,box1_x2,box1_y2\n",
    "    # box = (x1, y1, x2, y2)\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "\n",
    "    # obtain x1, y1, x2, y2 of the intersection\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    # compute the width and height of the intersection\n",
    "    w = max(0, x2 - x1 + 1)\n",
    "    h = max(0, y2 - y1 + 1)\n",
    "\n",
    "    inter = w * h\n",
    "    self_iou = inter / box2_area\n",
    "    return self_iou\n",
    "def detect():\n",
    "    source, weights, view_img, save_txt, imgsz, trace = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size, not opt.no_trace\n",
    "    save_img = not opt.nosave and not source.endswith('.txt')  # save inference images\n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "    box1_x1,box1_y1,box1_x2,box1_y2 = 0,0,0,0\n",
    "    top_iou_for10fps = []\n",
    "    iou_list = []\n",
    "    top_iou_obj = None\n",
    "    fps_cnt = 0.0\n",
    "    total_fps_cnt = 0.0\n",
    "    top_iou = None\n",
    "    ear = None\n",
    "    study_obj = ['book','laptop',]\n",
    "    # Initialize\n",
    "    set_logging()\n",
    "    device = select_device(opt.device)\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)# load FP32 model\n",
    "    stride = int(model.stride.max())  # model stride\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "\n",
    "    if trace:\n",
    "        model = TracedModel(model, device, opt.img_size)\n",
    "\n",
    "    if half:\n",
    "        model.half()  # to FP16\n",
    "\n",
    "    # Second-stage classifier\n",
    "    classify = False\n",
    "    if classify:\n",
    "        modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
    "\n",
    "    # Set Dataloader\n",
    "    vid_path, vid_writer = None, None\n",
    "    if webcam:\n",
    "        view_img = check_imshow()\n",
    "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "    # Get names and colors\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names\n",
    "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "\n",
    "    # Run inference\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "    old_img_w = old_img_h = imgsz\n",
    "    old_img_b = 1\n",
    "\n",
    "    t0 = time.time()\n",
    "    first_cnt=0\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        first_cnt+=1\n",
    "        if first_cnt==1:\n",
    "            if vid_cap:\n",
    "                fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            else:  # stream\n",
    "                fps, w, h = 30, im0s.shape[1], im0s.shape[0]\n",
    "            save_path ='run.mp4'    \n",
    "            out = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "\n",
    "        \n",
    "        with mp_face_mesh.FaceMesh( max_num_faces=1, refine_landmarks=True,\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "            im0s.flags.writeable = False\n",
    "            im0s = cv2.cvtColor(im0s, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(im0s)\n",
    "            x = im0s.shape[1] # height\n",
    "            y = im0s.shape[0] # width\n",
    "            book_head = y*1/4 \n",
    "\n",
    "            # Draw the face mesh annotations on the image.\n",
    "            im0s.flags.writeable = True\n",
    "            im0s = cv2.cvtColor(im0s, cv2.COLOR_RGB2BGR)   \n",
    "\n",
    "            if results.multi_face_landmarks:\n",
    "                for face_landmarks in (results.multi_face_landmarks):\n",
    "                    begin = time.time()\n",
    "                    # Drawing base line(facemesh)\n",
    "                    # eyes\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=im0s,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=FACEMESH_CONTOURS,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "                    # irises\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=im0s,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                        # mp_face_mesh\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n",
    "                    total_landmarks.append(face_landmarks.landmark)\n",
    "\n",
    "                    # Make DataFrames------------------------------------------------------------\n",
    "                    # iris data frame\n",
    "                    irises=[] # temporary list\n",
    "                    for iris, _ in FACEMESH_IRISES:\n",
    "                        irises.append(iris)\n",
    "                    irises.sort() # order\n",
    "                    total = [] # to be iris dataframe\n",
    "                    for n,_ in enumerate(irises):\n",
    "                        n+=1\n",
    "                        # 좌표 x,y,z값 순서 각 4개씩 (오른쪽눈 < 왼쪽눈) \n",
    "                        if n <=len(FACEMESH_LEFT_IRIS):\n",
    "                            direction = 'right'\n",
    "                        else:\n",
    "                            n-=len(FACEMESH_LEFT_IRIS)\n",
    "                            direction = 'left'\n",
    "                        now = [_,direction ,face_landmarks.landmark[_].x,face_landmarks.landmark[_].y,face_landmarks.landmark[_].z] # info in this time\n",
    "                        total.append(now) \n",
    "                    iris_df = pd.DataFrame(total, columns = ['idx','dir','x','y','z']) # idx: landmark, dir: right/left\n",
    "                    \n",
    "                    # iris / normalized data => resize to origin and to int\n",
    "                    iris_df['x'] = iris_df['x']*x\n",
    "                    iris_df['y'] = iris_df['y']*y\n",
    "                    iris_df['x'] = iris_df['x'].astype('int64')\n",
    "                    iris_df['y'] = iris_df['y'].astype('int64')\n",
    "\n",
    "                    # eyes data frame\n",
    "                    eyes=[] # temporary list\n",
    "                    for eye, _ in FACEMESH_EYES:\n",
    "                        eyes.append(eye)\n",
    "                        eyes.append(_)\n",
    "                    eyes = list(set(eyes))\n",
    "                    eyes.sort() # order\n",
    "                    total = [] # to be eyes dataframe\n",
    "                    for n,_ in enumerate(eyes):\n",
    "                        n+=1\n",
    "                        # 좌표 x,y,z값 순서 각 16개씩 (오른쪽눈 < 왼쪽눈) \n",
    "                        if n <= len(FACEMESH_LEFT_EYE): \n",
    "                            direction = 'right'     \n",
    "                        else:\n",
    "                            n-=int(len(FACEMESH_LEFT_EYE))\n",
    "                            direction = 'left'\n",
    "                        if _ in under:\n",
    "                            loc = 'under'\n",
    "                        else:\n",
    "                            loc = 'up'\n",
    "                        now = [_,direction ,face_landmarks.landmark[_].x,face_landmarks.landmark[_].y,face_landmarks.landmark[_].z,loc] # info in this time\n",
    "                        total.append(now)\n",
    "                    eyes_df = pd.DataFrame(total, columns = ['idx','dir','x','y','z','loc']) # idx: landmark, dir: right/left, loc: up/down\n",
    "                    \n",
    "                    # eyes / normalized data => resize to origin and to int\n",
    "                    eyes_df['x'] = eyes_df['x']*x\n",
    "                    eyes_df['y'] = eyes_df['y']*y\n",
    "                    eyes_df['x'] = eyes_df['x'].astype('int64')\n",
    "                    eyes_df['y'] = eyes_df['y'].astype('int64')\n",
    "                    \n",
    "                    # Gaze Point Estimation------------------------------------------------------------\n",
    "                    \n",
    "                    \n",
    "                    # 오른쪽 동공의 각 끝 좌표\n",
    "                    n469_x, n469_y = iris_df[iris_df['idx']==469].x,iris_df[iris_df['idx']==469].y\n",
    "                    n471_x, n471_y = iris_df[iris_df['idx']==471].x,iris_df[iris_df['idx']==471].y\n",
    "                    # 왼쪽 동공의 각 끝 좌표\n",
    "                    n474_x, n474_y = iris_df[iris_df['idx']==474].x,iris_df[iris_df['idx']==474].y\n",
    "                    n476_x, n476_y = iris_df[iris_df['idx']==476].x,iris_df[iris_df['idx']==476].y\n",
    "                    \n",
    "                    # 오른쪽 동공의 중심좌표\n",
    "                    dot_r = ((int(n469_x) + int(n471_x)) / 2, (int(n469_y) + int(n471_y)) / 2)\n",
    "                    # 왼쪽 동공의 중심좌표\n",
    "                    dot_l = ((int(n474_x) + int(n476_x)) / 2, (int(n474_y) + int(n476_y)) / 2)\n",
    "\n",
    "                    # 오른쪽 눈꺼풀의 각 끝 좌표와 길이\n",
    "                    n33 = (eyes_df[eyes_df['idx']==33].x,eyes_df[eyes_df['idx']==33].y)\n",
    "                    n133 = (eyes_df[eyes_df['idx']==133].x,eyes_df[eyes_df['idx']==133].y) \n",
    "                    # dist_r = math.dist(n33,n133)\n",
    "                    dist_r = distance(eyes_df[eyes_df['idx']==33].iloc[0].x,eyes_df[eyes_df['idx']==33].iloc[0].y,eyes_df[eyes_df['idx']==133].iloc[0].x,eyes_df[eyes_df['idx']==133].iloc[0].y)\n",
    "                    \n",
    "                    # 왼쪽 눈꺼풀의 각 끝 좌표와 길이\n",
    "                    n263 = (eyes_df[eyes_df['idx']==263].x,eyes_df[eyes_df['idx']==263].y)\n",
    "                    n362 = (eyes_df[eyes_df['idx']==362].x,eyes_df[eyes_df['idx']==362].y)\n",
    "                    # dist_l = math.dist(n263,n362)\n",
    "                    dist_l = distance(eyes_df[eyes_df['idx']==263].iloc[0].x,eyes_df[eyes_df['idx']==263].iloc[0].y,eyes_df[eyes_df['idx']==362].iloc[0].x,eyes_df[eyes_df['idx']==362].iloc[0].y)\n",
    "\n",
    "\n",
    "                    # 오른쪽 밑 눈꺼풀\n",
    "                    n145 = (eyes_df[eyes_df['idx']==145].x,eyes_df[eyes_df['idx']==145].y)\n",
    "                    # 왼쪽 밑 눈꺼풀\n",
    "                    n374 = (eyes_df[eyes_df['idx']==374].x,eyes_df[eyes_df['idx']==374].y)\n",
    "                    \n",
    "                    # gaze point line val\n",
    "                    # 눈 좌표 값 방향기준\n",
    "                    \n",
    "                    range_w = int(x*.07) # 좌측부터 2,3번째 그리드의 x좌표 간격에 각각 +,- 값 \n",
    "\n",
    "                    # gaze_point_line --------------------------------------------------\n",
    "                    right_line_x = ((n33[0][1]-range_w)/2)/2\n",
    "                    rightcenter_line_x = ((n33[0][1]-range_w)/2) + ((n33[0][1]-range_w)/2)/2\n",
    "                    center_line_x = (n263[0][17]+range_w - (n33[0][1]-range_w))/2 + (n33[0][1]-range_w)\n",
    "                    leftcenter_line_x = (n263[0][17]+range_w)+(x-(n263[0][17]+range_w))/4\n",
    "                    left_line_x = (n263[0][17]+range_w) + (x-(n263[0][17]+range_w))*3/4\n",
    "                    \n",
    "                    up_line_y = eyes_df[eyes_df['idx']==33].y[1]/2\n",
    "                    middle_line_y = eyes_df[eyes_df['idx']==33].y[1] + (y*.75 -  eyes_df[eyes_df['idx']==33].y[1])/2\n",
    "                    down_line_y = y*.75+y*.125\n",
    "                    \n",
    "                    # 오른쪽 눈 방향 (좌우)\n",
    "                    # r_ratio = round((math.dist(dot_r, n133)/dist_r),5) # if ratio < thres: left\n",
    "                    r_ratio = round(distance((int(n469_x)+int(n471_x))/2,(int(n469_y)+int(n471_y))/2,eyes_df[eyes_df['idx']==133].iloc[0].x,eyes_df[eyes_df['idx']==133].iloc[0].y)/dist_r,5)\n",
    "                    if r_ratio:\n",
    "                        if r_ratio < thres:\n",
    "                            dir_r = 'Right'\n",
    "                        elif r_ratio > thres_:\n",
    "                            dir_r = 'Left'\n",
    "                        else:\n",
    "                            dir_r = 'Center'\n",
    "                    # 왼쪽 눈 방향 (좌우)\n",
    "                    # l_ratio = round((math.dist(dot_l, n263)/dist_l),5) # if ratio < thres: left                \n",
    "                    l_ratio = round(distance((int(n474_x) + int(n476_x)) / 2, (int(n474_y) + int(n476_y)) / 2,eyes_df[eyes_df['idx']==263].iloc[0].x,eyes_df[eyes_df['idx']==263].iloc[0].y)/dist_l,5)\n",
    "                    if l_ratio:\n",
    "                        if l_ratio < thres:\n",
    "                            dir_l = 'Right'\n",
    "                        elif l_ratio > thres_:\n",
    "                            dir_l = 'Left'\n",
    "                        else:\n",
    "                            dir_l = 'Center'\n",
    "\n",
    "                    # 통합 눈 방향 (좌우)\n",
    "                    if dir_r == dir_l:\n",
    "                        dir_ = dir_r\n",
    "                        if dir_r == 'Right':\n",
    "                            gaze_line_x = left_line_x\n",
    "                            box1_x1, box1_x2 = int((x-(n263[0][17]+range_w))/2+(n263[0][17]+range_w)), x\n",
    "                        else:\n",
    "                            gaze_line_x = right_line_x\n",
    "                            box1_x1, box1_x2 = 0, int((n33[0][1]-range_w)/2)\n",
    "\n",
    "                    elif ((dir_r =='Right') and (dir_l =='Left')) or ((dir_r == 'Left') and (dir_l == 'Right')):\n",
    "                        dir_ = 'Center' # 양 끝 값일 때, 중앙으로\n",
    "                        gaze_line_x = center_line_x\n",
    "                        box1_x1, box1_x2 = n33[0][1]-range_w, n263[0][17]+range_w\n",
    "                    else: # [rightcenter, leftcenter, centerright, centerleft]\n",
    "                        dir_ = [dir_r,dir_l]\n",
    "                        if ('Right' in dir_) and ('Center' in dir_):\n",
    "                            dir_ = 'RightCenter'\n",
    "                            gaze_line_x = leftcenter_line_x\n",
    "                            box1_x1, box1_x2 = n263[0][17]+range_w,int((x-(n263[0][17]+range_w))/2+(n263[0][17]+range_w))\n",
    "                        if ('Left' in dir_) and ('Center' in dir_):\n",
    "                            dir_ = 'LeftCenter'\n",
    "                            gaze_line_x = rightcenter_line_x\n",
    "                            box1_x1, box1_x2 = int((n33[0][1]-range_w)/2),n33[0][1]-range_w\n",
    "            #                 up_r = iris_df[iris_df['idx']==472]['y'][3] - eyes_df[eyes_df['idx']==145].y[4] # if up<0: up\n",
    "            #                 up_l = iris_df[iris_df['idx']==477]['y'][7] - eyes_df[eyes_df['idx']==374].y[20] # if up<0: up\n",
    "\n",
    "                    # EAR ratio--------------------------------------------------\n",
    "                    # 오른쪽 눈 방향 (상하) : (|161-163|+|157-154|)/2*|133-33|*1/100\n",
    "                    n161 = (eyes_df[eyes_df['idx']==161].x,eyes_df[eyes_df['idx']==161].y)\n",
    "                    n163 = (eyes_df[eyes_df['idx']==163].x,eyes_df[eyes_df['idx']==163].y)\n",
    "                    n154 = (eyes_df[eyes_df['idx']==154].x,eyes_df[eyes_df['idx']==154].y)\n",
    "                    n157 = (eyes_df[eyes_df['idx']==157].x,eyes_df[eyes_df['idx']==157].y)\n",
    "                    # right_ear = (abs(math.dist(n161,n163))+abs(math.dist(n157,n154)))/2*abs(math.dist(n133,n33))/1000\n",
    "                    right_ear = (abs(distance(eyes_df[eyes_df['idx']==161].iloc[0].x,eyes_df[eyes_df['idx']==161].iloc[0].y,eyes_df[eyes_df['idx']==163].iloc[0].x,eyes_df[eyes_df['idx']==163].iloc[0].y))+\\\n",
    "                                  abs(distance(eyes_df[eyes_df['idx']==157].iloc[0].x,eyes_df[eyes_df['idx']==157].iloc[0].y,eyes_df[eyes_df['idx']==154].iloc[0].x,eyes_df[eyes_df['idx']==154].iloc[0].y)))/2*\\\n",
    "                                  abs(distance(eyes_df[eyes_df['idx']==133].iloc[0].x,eyes_df[eyes_df['idx']==133].iloc[0].y,eyes_df[eyes_df['idx']==33].iloc[0].x,eyes_df[eyes_df['idx']==33].iloc[0].y))/1000\n",
    "                    # 왼쪽 눈 방향 (상하) : (|384-381|+|388-390|)/2*|263-362|*1/100\n",
    "                    n381 = (eyes_df[eyes_df['idx']==381].x,eyes_df[eyes_df['idx']==381].y)\n",
    "                    n384 = (eyes_df[eyes_df['idx']==384].x,eyes_df[eyes_df['idx']==384].y)\n",
    "                    n388 = (eyes_df[eyes_df['idx']==388].x,eyes_df[eyes_df['idx']==388].y)\n",
    "                    n390 = (eyes_df[eyes_df['idx']==390].x,eyes_df[eyes_df['idx']==390].y)\n",
    "                    # left_ear = (abs(math.dist(n384,n381))+abs(math.dist(n388,n390)))/2*abs(math.dist(n263,n362))/1000\n",
    "                    left_ear = (abs(distance(eyes_df[eyes_df['idx']==381].iloc[0].x,eyes_df[eyes_df['idx']==381].iloc[0].y,eyes_df[eyes_df['idx']==384].iloc[0].x,eyes_df[eyes_df['idx']==384].iloc[0].y))+\\\n",
    "                                abs(distance(eyes_df[eyes_df['idx']==388].iloc[0].x,eyes_df[eyes_df['idx']==388].iloc[0].y,eyes_df[eyes_df['idx']==390].iloc[0].x,eyes_df[eyes_df['idx']==390].iloc[0].y)))/2*\\\n",
    "                                abs(distance(eyes_df[eyes_df['idx']==263].iloc[0].x,eyes_df[eyes_df['idx']==263].iloc[0].y,eyes_df[eyes_df['idx']==362].iloc[0].x,eyes_df[eyes_df['idx']==362].iloc[0].y))/1000\n",
    "                    # Right iris(468) z vs Left iris(473) z: higher value is closer camera.\n",
    "                    if face_landmarks.landmark[468].z > face_landmarks.landmark[473].z:\n",
    "                        using_ear = right_ear\n",
    "                    else:\n",
    "                        using_ear = left_ear\n",
    "\n",
    "                    if using_ear <= 0.15:\n",
    "                        ear = 'CLOSE'\n",
    "                        box1_y1, box1_y2 = int(y*0.75), y # down과 같음\n",
    "                        gaze_line_y = down_line_y\n",
    "                    elif (using_ear > 0.15) and (using_ear <= thres_ear/2):# thres_ear_ = thres_ear/2\n",
    "                        ear = 'DOWN'\n",
    "                        gaze_line_y = down_line_y\n",
    "                        box1_y1, box1_y2 = int(y*0.75), y\n",
    "                    elif (using_ear > 0.4) and (using_ear < thres_ear): # thres_ear = 0.7\n",
    "                        ear = 'MIDDLE'\n",
    "                        gaze_line_y = middle_line_y\n",
    "                        box1_y1, box1_y2 = eyes_df[eyes_df['idx']==33].y[1], int(y*0.75)\n",
    "                    else:\n",
    "                        ear = 'UP'\n",
    "                        gaze_line_y = up_line_y\n",
    "                        box1_y1, box1_y2 = 0,eyes_df[eyes_df['idx']==33].y[1]\n",
    "                        \n",
    "                img = torch.from_numpy(img).to(device)\n",
    "                img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "                img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "                if img.ndimension() == 3:\n",
    "                    img = img.unsqueeze(0)\n",
    "\n",
    "                # Warmup\n",
    "                if device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):\n",
    "                    old_img_b = img.shape[0]\n",
    "                    old_img_h = img.shape[2]\n",
    "                    old_img_w = img.shape[3]\n",
    "                    for i in range(3):\n",
    "                        model(img, augment=opt.augment)[0]\n",
    "\n",
    "                # Inference\n",
    "                t1 = time_synchronized()\n",
    "                pred = model(img, augment=opt.augment)[0]\n",
    "                t2 = time_synchronized()\n",
    "\n",
    "                # Apply NMS\n",
    "                pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "                t3 = time_synchronized()\n",
    "\n",
    "                # Apply Classifier\n",
    "                if classify:\n",
    "                    pred = apply_classifier(pred, modelc, img, im0s)\n",
    "                \n",
    "                if results.multi_face_landmarks:\n",
    "\n",
    "                    # Grid line--------------------------------------------------\n",
    "                    # out.write(im0s)\n",
    "                    # right 옆 - 왼쪽에서부터 2\n",
    "                    cv2.line(im0s,(n33[0][1]-range_w,0),(n33[0][1]-range_w,y),(255,0,0),1) # n33[0][1]= n33_x, range_w = 50\n",
    "                    # left 옆 - 3\n",
    "                    cv2.line(im0s,(n263[0][17]+range_w,0),(n263[0][17]+range_w,y),(255,0,0),1)\n",
    "\n",
    "                    # right center - 1 \n",
    "                    cv2.line(im0s,(int((n33[0][1]-range_w)/2),0),(int((n33[0][1]-range_w)/2),y),(255,0,0),1)\n",
    "                    # left center - 4\n",
    "                    cv2.line(im0s,(int((x-(n263[0][17]+range_w))/2+(n263[0][17]+range_w)),0),(int((x-(n263[0][17]+range_w))/2+(n263[0][17]+range_w)),y),(255,0,0),1) # n263[0][17]= n263_x\n",
    "                    \n",
    "                    # table\n",
    "                    cv2.line(im0s,(0,int(y*0.75)),(x,int(y*0.75)),(255,0,0),1)\n",
    "                    # eye_line \n",
    "                    cv2.line(im0s,(0,eyes_df[eyes_df['idx']==33].y[1]),(x,eyes_df[eyes_df['idx']==33].y[1]),(255,0,0),1) \n",
    "                    #cv2.line(im0s,(0,int(face_landmarks.landmark[10].y*y)),(x,int(face_landmarks.landmark[10].y*y)),(255,0,0),3) # 이마라인선 but, down과 middle의 기준이 애매함, 눈꼬리 기준으로 위아래 나누는게 더 좋을듯\n",
    "                    \n",
    "                    # gaze point line --------------------------------------------------\n",
    "                            # print(gaze_line_x)\n",
    "                            # print(gaze_line_y)\n",
    "\n",
    "\n",
    "                    # put text --------------------------------------------------   \n",
    "                    if dir_:\n",
    "                        org=(int(x*0.3),int(y*0.3))\n",
    "                        font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                        cv2.putText(im0s,dir_,org,font,.5,(255,0,0),1)\n",
    "                        # size, BaseLine=cv2.getTextSize(dir_,font,1,2)\n",
    "                    if ear:\n",
    "                        org=(int(x*0.3),int(y*0.4))\n",
    "                        font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                        cv2.putText(im0s,ear,org,font,.5,(255,0,0),1)\n",
    "                        # size, BaseLine=cv2.getTextSize(ear,font,1,2)\n",
    "                iou_key = []\n",
    "                iou_val = []\n",
    "                # Process detections\n",
    "                for i, det in enumerate(pred):  # detections per image\n",
    "                    \n",
    "                    if webcam:  # batch_size >= 1\n",
    "                        p, s, frame = path[i], '%g: ' % i, dataset.count\n",
    "                    else:\n",
    "                        p, s, frame = path, '', getattr(dataset, 'frame', 0)\n",
    "\n",
    "                    # gn = torch.tensor(im0s.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "                    if len(det):\n",
    "                        # Rescale boxes from img_size to im0 size\n",
    "                        det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n",
    "\n",
    "                        # Print results   \n",
    "                        for c in det[:, -1].unique():\n",
    "                            n = (det[:, -1] == c).sum()  # detections per class\n",
    "                            s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "                        \n",
    "    # --------------------------------------------------------------------------------------------------------- 1 frame 내의 bbox 모두\n",
    "                        iou_key = []\n",
    "                        iou_val = []\n",
    "                        iou_xyxy = []\n",
    "                        cell_phone_xyxy = 0\n",
    "                        for  *xyxy, conf, cls in reversed(det):\n",
    "                          \n",
    "                          xyxy_ = []\n",
    "                          for _ in xyxy:\n",
    "                              xyxy_.append(_.item())\n",
    "                          # print(xyxy_)\n",
    "\n",
    "                          # 사용물품이 화면 중앙에서 사용 될 때, 탐지 X 보완(ex. cell phone)\n",
    "                          if names[int(cls.item())] =='book':\n",
    "                              book_head = xyxy_[1] # 공부X 물품 기준선\n",
    "\n",
    "                          if (names[int(cls.item())] =='cell phone') and (xyxy_[1] < book_head):\n",
    "                              cell_phone_xyxy = xyxy # 휴대폰의 위치\n",
    "\n",
    "                          # print(\"cls:\",names[int(cls.item())])\n",
    "                          label = f'{names[int(cls)]} {conf:.2f}'\n",
    "                          # 객체별 bbox 그리기\n",
    "                          plot_one_box(xyxy, im0s, label=label, color=colors[int(cls)], line_thickness=1)\n",
    "\n",
    "                          # iou\n",
    "                          if box1_x1 or box1_y1 or box1_x2 or box1_y2:\n",
    "                              box1 = [box1_x1, box1_y1, box1_x2, box1_y2]\n",
    "                              box2 = xyxy_\n",
    "                              iou = IoU(box1,box2)\n",
    "                              iou_key.append(names[int(cls.item())])\n",
    "                              iou_val.append(iou)\n",
    "                              iou_xyxy.append(xyxy_)\n",
    "                          \n",
    "                          # 들여쓰기 주의\n",
    "                        if iou_key or iou_val:\n",
    "                            top_iou = iou_key[np.argmax(iou_val)] # 1 frame의 가장 높은 값 명사로 저장됨(그리드 기준)\n",
    "                            \n",
    "                            if cell_phone_xyxy: # 예외 정보(사용물품 화면중앙에서 사용 될 때의 보완점)\n",
    "                                top_iou = 'cell phone'\n",
    "                            \n",
    "                            top_iou_for10fps.append(top_iou)\n",
    "\n",
    "                        if top_iou: # 현재 보는 거\n",
    "                            org=(int(x*0.1),int(y*0.1))\n",
    "                            font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                            cv2.putText(im0s,'NOW: '+top_iou, org, font,.5,(255,0,0),1)\n",
    "\n",
    "                        # writing => top_iou_obj(10fps동안 빈도수 1등)\n",
    "                        if top_iou_obj: \n",
    "                            org=(int(x*0.1),int(y*0.2))\n",
    "                            font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                            cv2.putText(im0s,top_iou_obj+' in 10FPS',org,font,.5,(255,0,0),1)    \n",
    "\n",
    "                        # 10개의 프레임 중에 가장 높은 사물\n",
    "                        if top_iou_for10fps:\n",
    "                            counter_top_iou = Counter(top_iou_for10fps)\n",
    "                            top_iou_obj = list(counter_top_iou.keys())[(np.argmax(list(counter_top_iou.values())))] # 명사로 저장됨\n",
    "                            # print('top_iou_for10fps :',top_iou_for10fps)\n",
    "                            # print('top_iou_obj:',top_iou_obj)\n",
    "                        # 최근 10개의 프레임\n",
    "                        if len(top_iou_for10fps) == 10: \n",
    "                            top_iou_for10fps = top_iou_for10fps[1:]                      \n",
    "                        \n",
    "                        if top_iou_obj in study_obj: \n",
    "                            fps_cnt += 1/30 # 순공시간, 1단위: 1초\n",
    "                            # print('{}m {}s {}ms'.format(fps_cnt//60,fps_cnt//1, fps_cnt%1))                \n",
    "\n",
    "                        if fps_cnt: # 현재 시간\n",
    "                            org=(int(x*0.2),int(y*0.1))\n",
    "                            font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                            cv2.putText(im0s,'now: {}:{}:{:.3f}'.format(int(fps_cnt//60),int(fps_cnt//1),fps_cnt%1), org, font,.5,(255,0,0),1)              \n",
    "                                  \n",
    "                        total_fps_cnt += 1/30 # 전체시간\n",
    "                        if total_fps_cnt: # 전체시간\n",
    "                            org=(int(x*0.4),int(y*0.1))\n",
    "                            font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                            cv2.putText(im0s,'total: {}:{}:{:.3f}'.format(int(total_fps_cnt//60),int(total_fps_cnt//1),total_fps_cnt%1), org, font,.5,(255,0,0),1)                    \n",
    "                    \n",
    "    # gaze point line\n",
    "                        if ear != 'UP':\n",
    "                          # if (xyxy_[1] < book_head) and (names[int(cls.item())] =='cell phone'):\n",
    "                          # if (iou_xyxy[np.argmax(iou_val)][1] < book_head):\n",
    "                          if cell_phone_xyxy:\n",
    "                            if top_iou_obj =='cell phone':\n",
    "                              cv2.line(im0s,(int(face_landmarks.landmark[468].x * x),int(face_landmarks.landmark[468].y * y)),\n",
    "                                      (int((cell_phone_xyxy[0] + cell_phone_xyxy[2]) / 2 - x * .02),int((cell_phone_xyxy[1] + cell_phone_xyxy[3]) / 2))\n",
    "                                      ,(255,0,0),2)\n",
    "                              cv2.line(im0s,(int(face_landmarks.landmark[473].x * x),int(face_landmarks.landmark[473].y * y)),\n",
    "                                      (int((cell_phone_xyxy[0] + cell_phone_xyxy[2]) / 2 + x * .02),int((cell_phone_xyxy[1] + cell_phone_xyxy[3]) / 2))\n",
    "                                      ,(255,0,0),2)                                                \n",
    "                              \n",
    "                            out.write(im0s)\n",
    "                            # cv2_imshow(im0s)  \n",
    "\n",
    "                          else:\n",
    "                            cv2.line(im0s,(int(face_landmarks.landmark[468].x*x),int(face_landmarks.landmark[468].y*y)),(int(gaze_line_x-x*.07), int(gaze_line_y)),(255,0,0),2) \n",
    "                            cv2.line(im0s,(int(face_landmarks.landmark[473].x*x),int(face_landmarks.landmark[473].y*y)),(int(gaze_line_x+x*.07), int(gaze_line_y)),(255,0,0),2)\n",
    "                            \n",
    "                            out.write(im0s)\n",
    "                            # cv2_imshow(im0s)                      \n",
    "          \n",
    "    # --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                    # Print time (inference + NMS)\n",
    "                    print(f'{s}Done. ({(1E3 * (t2 - t1)):.1f}ms) Inference, ({(1E3 * (t3 - t2)):.1f}ms) NMS')\n",
    "            else: # facemesh 안될 때\n",
    "                total_fps_cnt += 1/30\n",
    "                if total_fps_cnt:\n",
    "                    org=(int(x*0.4),int(y*0.1))\n",
    "                    font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    cv2.putText(im0s,'total: {}:{}:{:.3f}'.format(int(total_fps_cnt//60),int(total_fps_cnt//1),total_fps_cnt%1), org, font,.5,(255,0,0),1)                    \n",
    "                out.write(im0s)\n",
    "                                \n",
    "    vid_cap.release()\n",
    "    out.release()\n",
    "15//7\n",
    "2\n",
    "\n",
    "a = ['a','b','a']\n",
    "if a[0]:\n",
    "  print(a)\n",
    "['a', 'b', 'a']\n",
    "\n",
    "# 실행문\n",
    "with torch.no_grad():\n",
    "    if opt.update:  # update all models (to fix SourceChangeWarning)\n",
    "        for opt.weights in ['yolov7.pt']:\n",
    "            detect()\n",
    "            strip_optimizer(opt.weights)\n",
    "    else:\n",
    "        detect()\n",
    "Fusing layers... \n",
    "RepConv.fuse_repvgg_block\n",
    "RepConv.fuse_repvgg_block\n",
    "RepConv.fuse_repvgg_block\n",
    " Convert model to Traced-model... \n",
    " traced_script_module saved! \n",
    " model is traced! \n",
    "\n",
    "WARNING: Environment does not support cv2.imshow() or PIL Image.show() image displays\n",
    "cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\n",
    "to crash; see https://github.com/jupyter/notebook/issues/3935.\n",
    "As a substitution, consider using\n",
    "  from google.colab.patches import cv2_imshow\n",
    "\n",
    "1/1: 0... \n",
    "---------------------------------------------------------------------------\n",
    "AssertionError                            Traceback (most recent call last)\n",
    "<ipython-input-119-597c23014cc1> in <module>\n",
    "      3     if opt.update:  # update all models (to fix SourceChangeWarning)\n",
    "      4         for opt.weights in ['yolov7.pt']:\n",
    "----> 5             detect()\n",
    "      6             strip_optimizer(opt.weights)\n",
    "      7     else:\n",
    "\n",
    "<ipython-input-116-660b225e91a4> in detect()\n",
    "     40         view_img = check_imshow()\n",
    "     41         cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "---> 42         dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "     43     else:\n",
    "     44         dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "/content/drive/MyDrive/yolov7/utils/datasets.py in __init__(self, sources, img_size, stride)\n",
    "    288                 url = pafy.new(url).getbest(preftype=\"mp4\").url\n",
    "    289             cap = cv2.VideoCapture(url)\n",
    "--> 290             assert cap.isOpened(), f'Failed to open {s}'\n",
    "    291             w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    292             h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "AssertionError: Failed to open 0\n",
    "(32 * 1/30)//1\n",
    "1.0\n",
    "24*30\n",
    "720\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
