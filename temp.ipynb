{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e863766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolov7 base import & variables\n",
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "import easydict\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
    "opt = easydict.EasyDict({\n",
    "    \"weights\":'yolov7-e6.pt',\n",
    "    \"source\":'vd000.mp4',\n",
    "    \n",
    "    \"img_size\":640,\n",
    "    \"conf_thres\":0.25,\n",
    "    \"iou_thres\":0.45,\n",
    "    \"device\":'',\n",
    "    \"view_img\":False,\n",
    "    \"save_txt\":False,\n",
    "    \"save_conf\":False,\n",
    "    \"nosave\":False,\n",
    "    \"classes\":None,\n",
    "    \n",
    "    \"agnostic_nms\":False,\n",
    "    \"augment\":False,\n",
    "    \"updata\":False,\n",
    "    \"project\":'runs/detect',\n",
    "    \"name\":'exp',\n",
    "    \"exist_ok\":False,\n",
    "    \"no_trace\":False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424b190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74610aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect():\n",
    "    source, weights, view_img, save_txt, imgsz, trace = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size, not opt.no_trace\n",
    "    save_img = not opt.nosave and not source.endswith('.txt')  # save inference images\n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "\n",
    "    # Initialize\n",
    "    set_logging()\n",
    "    device = select_device(opt.device)\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)# load FP32 model\n",
    "    stride = int(model.stride.max())  # model stride\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "\n",
    "    if trace:\n",
    "        model = TracedModel(model, device, opt.img_size)\n",
    "\n",
    "    if half:\n",
    "        model.half()  # to FP16\n",
    "\n",
    "    # Second-stage classifier\n",
    "    classify = False\n",
    "    if classify:\n",
    "        modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
    "\n",
    "    # Set Dataloader\n",
    "    vid_path, vid_writer = None, None\n",
    "    if webcam:\n",
    "        view_img = check_imshow()\n",
    "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "    # Get names and colors\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names\n",
    "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "\n",
    "    # Run inference\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "    old_img_w = old_img_h = imgsz\n",
    "    old_img_b = 1\n",
    "    t0 = time.time()\n",
    "    first_cnt=0\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        first_cnt+=1\n",
    "        if first_cnt==1:\n",
    "            if vid_cap:\n",
    "                fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            else:  # stream\n",
    "                fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "            save_path ='run.mp4'    \n",
    "            out = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "\n",
    "        \n",
    "        with mp_face_mesh.FaceMesh( max_num_faces=1, refine_landmarks=True,\n",
    "        min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "            im0s.flags.writeable = False\n",
    "            im0s = cv2.cvtColor(im0s, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(im0s)\n",
    "            x = im0s.shape[1] # height\n",
    "            y = im0s.shape[0] # width\n",
    "\n",
    "            # Draw the face mesh annotations on the image.\n",
    "            im0s.flags.writeable = True\n",
    "            im0s = cv2.cvtColor(im0s, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.multi_face_landmarks:\n",
    "                for face_landmarks in (results.multi_face_landmarks):\n",
    "                    begin = time.time()\n",
    "                    # Drawing base line(facemesh)\n",
    "                    # eyes\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=im0s,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=FACEMESH_CONTOURS,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "                    # irises\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=im0s,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                        # mp_face_mesh\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n",
    "                    total_landmarks.append(face_landmarks.landmark)\n",
    "                    # Make DataFrames------------------------------------------------------------\n",
    "                    # iris data frame\n",
    "                    irises=[] # temporary list\n",
    "                    for iris, _ in FACEMESH_IRISES:\n",
    "                        irises.append(iris)\n",
    "                    irises.sort() # order\n",
    "                    total = [] # to be iris dataframe\n",
    "                    for n,_ in enumerate(irises):\n",
    "                        n+=1\n",
    "                        # 좌표 x,y,z값 순서 각 4개씩 (오른쪽눈 < 왼쪽눈) \n",
    "                        if n <=len(FACEMESH_LEFT_IRIS):\n",
    "                            direction = 'right'\n",
    "                        else:\n",
    "                            n-=len(FACEMESH_LEFT_IRIS)\n",
    "                            direction = 'left'\n",
    "                        now = [_,direction ,face_landmarks.landmark[_].x,face_landmarks.landmark[_].y,face_landmarks.landmark[_].z] # info in this time\n",
    "                        total.append(now) \n",
    "                    iris_df = pd.DataFrame(total, columns = ['idx','dir','x','y','z']) # idx: landmark, dir: right/left\n",
    "                    \n",
    "                    # iris / normalized data => resize to origin and to int\n",
    "                    iris_df['x'] = iris_df['x']*x\n",
    "                    iris_df['y'] = iris_df['y']*y\n",
    "                    iris_df['x'] = iris_df['x'].astype('int64')\n",
    "                    iris_df['y'] = iris_df['y'].astype('int64')\n",
    "\n",
    "                    # eyes data frame\n",
    "                    eyes=[] # temporary list\n",
    "                    for eye, _ in FACEMESH_EYES:\n",
    "                        eyes.append(eye)\n",
    "                        eyes.append(_)\n",
    "                    eyes = list(set(eyes))\n",
    "                    eyes.sort() # order\n",
    "                    total = [] # to be eyes dataframe\n",
    "                    for n,_ in enumerate(eyes):\n",
    "                        n+=1\n",
    "                        # 좌표 x,y,z값 순서 각 16개씩 (오른쪽눈 < 왼쪽눈) \n",
    "                        if n <= len(FACEMESH_LEFT_EYE): \n",
    "                            direction = 'right'     \n",
    "                        else:\n",
    "                            n-=int(len(FACEMESH_LEFT_EYE))\n",
    "                            direction = 'left'\n",
    "                        if _ in under:\n",
    "                            loc = 'under'\n",
    "                        else:\n",
    "                            loc = 'up'\n",
    "                        now = [_,direction ,face_landmarks.landmark[_].x,face_landmarks.landmark[_].y,face_landmarks.landmark[_].z,loc] # info in this time\n",
    "                        total.append(now)\n",
    "                    eyes_df = pd.DataFrame(total, columns = ['idx','dir','x','y','z','loc']) # idx: landmark, dir: right/left, loc: up/down\n",
    "                    \n",
    "                    # eyes / normalized data => resize to origin and to int\n",
    "                    eyes_df['x'] = eyes_df['x']*x\n",
    "                    eyes_df['y'] = eyes_df['y']*y\n",
    "                    eyes_df['x'] = eyes_df['x'].astype('int64')\n",
    "                    eyes_df['y'] = eyes_df['y'].astype('int64')\n",
    "                    \n",
    "                    # Gaze Point Estimation------------------------------------------------------------\n",
    "                    # 오른쪽 동공의 각 끝 좌표\n",
    "                    n469_x, n469_y = iris_df[iris_df['idx']==469].x,iris_df[iris_df['idx']==469].y\n",
    "                    n471_x, n471_y = iris_df[iris_df['idx']==471].x,iris_df[iris_df['idx']==471].y\n",
    "                    # 왼쪽 동공의 각 끝 좌표\n",
    "                    n474_x, n474_y = iris_df[iris_df['idx']==474].x,iris_df[iris_df['idx']==474].y\n",
    "                    n476_x, n476_y = iris_df[iris_df['idx']==476].x,iris_df[iris_df['idx']==476].y\n",
    "                    \n",
    "                    # 오른쪽 동공의 중심좌표\n",
    "                    dot_r = ((int(n469_x) + int(n471_x)) / 2, (int(n469_y) + int(n471_y)) / 2)\n",
    "                    # 왼쪽 동공의 중심좌표\n",
    "                    dot_l = ((int(n474_x) + int(n476_x)) / 2, (int(n474_y) + int(n476_y)) / 2)\n",
    "\n",
    "                    # 오른쪽 눈꺼풀의 각 끝 좌표와 길이\n",
    "                    n33 = (eyes_df[eyes_df['idx']==33].x,eyes_df[eyes_df['idx']==33].y)\n",
    "                    n133 = (eyes_df[eyes_df['idx']==133].x,eyes_df[eyes_df['idx']==133].y) \n",
    "                    # dist_r = math.dist(n33,n133)\n",
    "                    dist_r = distance(eyes_df[eyes_df['idx']==33].iloc[0].x,eyes_df[eyes_df['idx']==33].iloc[0].y,eyes_df[eyes_df['idx']==133].iloc[0].x,eyes_df[eyes_df['idx']==133].iloc[0].y)\n",
    "                    \n",
    "                    # 왼쪽 눈꺼풀의 각 끝 좌표와 길이\n",
    "                    n263 = (eyes_df[eyes_df['idx']==263].x,eyes_df[eyes_df['idx']==263].y)\n",
    "                    n362 = (eyes_df[eyes_df['idx']==362].x,eyes_df[eyes_df['idx']==362].y)\n",
    "                    # dist_l = math.dist(n263,n362)\n",
    "                    dist_l = distance(eyes_df[eyes_df['idx']==263].iloc[0].x,eyes_df[eyes_df['idx']==263].iloc[0].y,eyes_df[eyes_df['idx']==362].iloc[0].x,eyes_df[eyes_df['idx']==362].iloc[0].y)\n",
    "\n",
    "\n",
    "                    # 오른쪽 밑 눈꺼풀\n",
    "                    n145 = (eyes_df[eyes_df['idx']==145].x,eyes_df[eyes_df['idx']==145].y)\n",
    "                    # 왼쪽 밑 눈꺼풀\n",
    "                    n374 = (eyes_df[eyes_df['idx']==374].x,eyes_df[eyes_df['idx']==374].y)\n",
    "                    \n",
    "                    # gaze point line val\n",
    "                    # 눈 좌표 값 방향기준\n",
    "                    \n",
    "                    range_w = int(x*.07) # 좌측부터 2,3번째 그리드의 x좌표 간격에 각각 +,- 값 \n",
    "\n",
    "                    # gaze_point_line --------------------------------------------------\n",
    "                    right_line_x = ((n33[0][1]-range_w)/2)/2\n",
    "                    rightcenter_line_x = ((n33[0][1]-range_w)/2) + ((n33[0][1]-range_w)/2)/2\n",
    "                    center_line_x = (n263[0][17]+range_w - (n33[0][1]-range_w))/2 + (n33[0][1]-range_w)\n",
    "                    leftcenter_line_x = (n263[0][17]+range_w)+(x-(n263[0][17]+range_w))/4\n",
    "                    left_line_x = (n263[0][17]+range_w) + (x-(n263[0][17]+range_w))*3/4\n",
    "                    \n",
    "                    up_line_y = eyes_df[eyes_df['idx']==33].y[1]/2\n",
    "                    middle_line_y = eyes_df[eyes_df['idx']==33].y[1] + (y*.75 -  eyes_df[eyes_df['idx']==33].y[1])/2\n",
    "                    down_line_y = y*.75+y*.125          \n",
    "                    # 오른쪽 눈 방향 (좌우)\n",
    "                    # r_ratio = round((math.dist(dot_r, n133)/dist_r),5) # if ratio < thres: left\n",
    "                    r_ratio = round(distance((int(n469_x)+int(n471_x))/2,(int(n469_y)+int(n471_y))/2,eyes_df[eyes_df['idx']==133].iloc[0].x,eyes_df[eyes_df['idx']==133].iloc[0].y)/dist_r,5)\n",
    "                    if r_ratio:\n",
    "                        if r_ratio < thres:\n",
    "                            dir_r = 'Right'\n",
    "                        elif r_ratio > thres_:\n",
    "                            dir_r = 'Left'\n",
    "                        else:\n",
    "                            dir_r = 'Center'\n",
    "                    # 왼쪽 눈 방향 (좌우)\n",
    "                    # l_ratio = round((math.dist(dot_l, n263)/dist_l),5) # if ratio < thres: left                \n",
    "                    l_ratio = round(distance((int(n474_x) + int(n476_x)) / 2, (int(n474_y) + int(n476_y)) / 2,eyes_df[eyes_df['idx']==263].iloc[0].x,eyes_df[eyes_df['idx']==263].iloc[0].y)/dist_l,5)\n",
    "                    if l_ratio:\n",
    "                        if l_ratio < thres:\n",
    "                            dir_l = 'Right'\n",
    "                        elif l_ratio > thres_:\n",
    "                            dir_l = 'Left'\n",
    "                        else:\n",
    "                            dir_l = 'Center'\n",
    "                    # 통합 눈 방향 (좌우)\n",
    "                    if dir_r == dir_l:\n",
    "                        dir_ = dir_r\n",
    "                        if dir_r == 'Right':\n",
    "                            gaze_line_x = left_line_x\n",
    "                        else:\n",
    "                            gaze_line_x = right_line_x\n",
    "                    elif ((dir_r =='Right') and (dir_l =='Left')) or ((dir_r == 'Left') and (dir_l == 'Right')):\n",
    "                        dir_ = 'Center' # 양 끝 값일 때, 중앙으로\n",
    "                        gaze_line_x = center_line_x\n",
    "                    else: # [rightcenter, leftcenter, centerright, centerleft]\n",
    "                        dir_ = [dir_r,dir_l]\n",
    "                        if ('Right' in dir_) and ('Center' in dir_):\n",
    "                            dir_ = 'RightCenter'\n",
    "                            gaze_line_x = leftcenter_line_x\n",
    "                        if ('Left' in dir_) and ('Center' in dir_):\n",
    "                            dir_ = 'LeftCenter'\n",
    "                            gaze_line_x = rightcenter_line_x\n",
    "\n",
    "            #                 up_r = iris_df[iris_df['idx']==472]['y'][3] - eyes_df[eyes_df['idx']==145].y[4] # if up<0: up\n",
    "            #                 up_l = iris_df[iris_df['idx']==477]['y'][7] - eyes_df[eyes_df['idx']==374].y[20] # if up<0: up\n",
    "                    # EAR ratio--------------------------------------------------\n",
    "                    # 오른쪽 눈 방향 (상하) : (|161-163|+|157-154|)/2*|133-33|*1/100\n",
    "                    n161 = (eyes_df[eyes_df['idx']==161].x,eyes_df[eyes_df['idx']==161].y)\n",
    "                    n163 = (eyes_df[eyes_df['idx']==163].x,eyes_df[eyes_df['idx']==163].y)\n",
    "                    n154 = (eyes_df[eyes_df['idx']==154].x,eyes_df[eyes_df['idx']==154].y)\n",
    "                    n157 = (eyes_df[eyes_df['idx']==157].x,eyes_df[eyes_df['idx']==157].y)\n",
    "                    # right_ear = (abs(math.dist(n161,n163))+abs(math.dist(n157,n154)))/2*abs(math.dist(n133,n33))/1000\n",
    "                    right_ear = (abs(distance(eyes_df[eyes_df['idx']==161].iloc[0].x,eyes_df[eyes_df['idx']==161].iloc[0].y,eyes_df[eyes_df['idx']==163].iloc[0].x,eyes_df[eyes_df['idx']==163].iloc[0].y))+\\\n",
    "                                  abs(distance(eyes_df[eyes_df['idx']==157].iloc[0].x,eyes_df[eyes_df['idx']==157].iloc[0].y,eyes_df[eyes_df['idx']==154].iloc[0].x,eyes_df[eyes_df['idx']==154].iloc[0].y)))/2*\\\n",
    "                                  abs(distance(eyes_df[eyes_df['idx']==133].iloc[0].x,eyes_df[eyes_df['idx']==133].iloc[0].y,eyes_df[eyes_df['idx']==33].iloc[0].x,eyes_df[eyes_df['idx']==33].iloc[0].y))/1000\n",
    "                    # 왼쪽 눈 방향 (상하) : (|384-381|+|388-390|)/2*|263-362|*1/100\n",
    "                    n381 = (eyes_df[eyes_df['idx']==381].x,eyes_df[eyes_df['idx']==381].y)\n",
    "                    n384 = (eyes_df[eyes_df['idx']==384].x,eyes_df[eyes_df['idx']==384].y)\n",
    "                    n388 = (eyes_df[eyes_df['idx']==388].x,eyes_df[eyes_df['idx']==388].y)\n",
    "                    n390 = (eyes_df[eyes_df['idx']==390].x,eyes_df[eyes_df['idx']==390].y)\n",
    "                    # left_ear = (abs(math.dist(n384,n381))+abs(math.dist(n388,n390)))/2*abs(math.dist(n263,n362))/1000\n",
    "                    left_ear = (abs(distance(eyes_df[eyes_df['idx']==381].iloc[0].x,eyes_df[eyes_df['idx']==381].iloc[0].y,eyes_df[eyes_df['idx']==384].iloc[0].x,eyes_df[eyes_df['idx']==384].iloc[0].y))+\\\n",
    "                                abs(distance(eyes_df[eyes_df['idx']==388].iloc[0].x,eyes_df[eyes_df['idx']==388].iloc[0].y,eyes_df[eyes_df['idx']==390].iloc[0].x,eyes_df[eyes_df['idx']==390].iloc[0].y)))/2*\\\n",
    "                                abs(distance(eyes_df[eyes_df['idx']==263].iloc[0].x,eyes_df[eyes_df['idx']==263].iloc[0].y,eyes_df[eyes_df['idx']==362].iloc[0].x,eyes_df[eyes_df['idx']==362].iloc[0].y))/1000\n",
    "                    # Right iris(468) z vs Left iris(473) z: higher value is closer camera.\n",
    "                    if face_landmarks.landmark[468].z > face_landmarks.landmark[473].z:\n",
    "                        using_ear = right_ear\n",
    "                    else:\n",
    "                        using_ear = left_ear\n",
    "                    if using_ear <= 0.15:\n",
    "                        ear = 'CLOSE'\n",
    "                        gaze_line_y = down_line_y\n",
    "                    elif (using_ear > 0.15) and (using_ear <= thres_ear/2):# thres_ear_ = thres_ear/2\n",
    "                        ear = 'DOWN'\n",
    "                        gaze_line_y = down_line_y\n",
    "                    elif (using_ear > 0.4) and (using_ear < thres_ear): # thres_ear = 0.7\n",
    "                        ear = 'MIDDLE'\n",
    "                        gaze_line_y = middle_line_y\n",
    "                    else:\n",
    "                        ear = 'UP'\n",
    "                        gaze_line_y = up_line_y\n",
    "                        \n",
    "\n",
    "                    # time --------------------------------------------------\n",
    "                    end = time.time()\n",
    "                    t = end - begin # 현재 frame 시간 값\n",
    "                    \n",
    "            #                 if obj in positive_list: # 공부하는 시간은 frame시간 더함\n",
    "            #                     study_time += t # 순공시간\n",
    "            #                 else:\n",
    "            #                     continue    \n",
    "            #                 now = [t,obj] # 현재 행\n",
    "            #                 time_list.append(now) # sum 했을 때 => 전체 시간 값\n",
    "\n",
    "\n",
    "            #             cv2.imshow('MediaPipe', cv2.flip(im0s, 0))         # Flip the image horizontally for a selfie-view display.\n",
    "                # cv2_imshow(im0s)\n",
    "            img = torch.from_numpy(img).to(device)\n",
    "            img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "            if img.ndimension() == 3:\n",
    "                img = img.unsqueeze(0)\n",
    "\n",
    "            # Warmup\n",
    "            if device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):\n",
    "                old_img_b = img.shape[0]\n",
    "                old_img_h = img.shape[2]\n",
    "                old_img_w = img.shape[3]\n",
    "                for i in range(3):\n",
    "                    model(img, augment=opt.augment)[0]\n",
    "\n",
    "            # Inference\n",
    "            t1 = time_synchronized()\n",
    "            pred = model(img, augment=opt.augment)[0]\n",
    "            t2 = time_synchronized()\n",
    "\n",
    "            # Apply NMS\n",
    "            pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "            t3 = time_synchronized()\n",
    "\n",
    "            # Apply Classifier\n",
    "            if classify:\n",
    "                pred = apply_classifier(pred, modelc, img, im0s)\n",
    "            \n",
    "            if results.multi_face_landmarks:\n",
    "\n",
    "                # Grid line--------------------------------------------------\n",
    "                # out.write(im0s)\n",
    "                cv2.line(im0s,(n33[0][1]-range_w,0),(n33[0][1]-range_w,y),(255,0,0),3) # n33[0][1]= n33_x, range_w = 50\n",
    "                cv2.line(im0s,(n263[0][17]+range_w,0),(n263[0][17]+range_w,y),(255,0,0),3)\n",
    "\n",
    "                cv2.line(im0s,(int((n33[0][1]-range_w)/2),0),(int((n33[0][1]-range_w)/2),y),(255,0,0),3)\n",
    "                cv2.line(im0s,(int((x-(n263[0][17]+range_w))/2+(n263[0][17]+range_w)),0),(int((x-(n263[0][17]+range_w))/2+(n263[0][17]+range_w)),y),(255,0,0),3) # n263[0][17]= n263_x\n",
    "                \n",
    "                cv2.line(im0s,(0,int(y*0.75)),(x,int(y*0.75)),(255,0,0),1) # 책상선\n",
    "                cv2.line(im0s,(0,eyes_df[eyes_df['idx']==33].y[1]),(x,eyes_df[eyes_df['idx']==33].y[1]),(255,0,0),1) # 오른쪽 바깥 눈꼬리 기준\n",
    "                #cv2.line(im0s,(0,int(face_landmarks.landmark[10].y*y)),(x,int(face_landmarks.landmark[10].y*y)),(255,0,0),3) # 이마라인선 but, down과 middle의 기준이 애매함, 눈꼬리 기준으로 위아래 나누는게 더 좋을듯\n",
    "                \n",
    "                # gaze point line --------------------------------------------------\n",
    "                        # print(gaze_line_x)\n",
    "                        # print(gaze_line_y)\n",
    "                if ear != 'UP':\n",
    "                    cv2.line(im0s,(int(face_landmarks.landmark[468].x*x),int(face_landmarks.landmark[468].y*y)),(int(gaze_line_x-x*.07), int(gaze_line_y)),(255,0,0),2) \n",
    "                    cv2.line(im0s,(int(face_landmarks.landmark[473].x*x),int(face_landmarks.landmark[473].y*y)),(int(gaze_line_x+x*.07), int(gaze_line_y)),(255,0,0),2)\n",
    "                # put text --------------------------------------------------   \n",
    "                if dir_:\n",
    "                    org=(int(x*0.3),int(y*0.3))\n",
    "                    font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    cv2.putText(im0s,dir_,org,font,.5,(255,0,0),1)\n",
    "                    # size, BaseLine=cv2.getTextSize(dir_,font,1,2)\n",
    "                if ear:\n",
    "                    org=(int(x*0.3),int(y*0.4))\n",
    "                    font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    cv2.putText(im0s,ear,org,font,.5,(255,0,0),1)\n",
    "                    # size, BaseLine=cv2.getTextSize(ear,font,1,2)\n",
    "\n",
    "            # Process detections\n",
    "            for i, det in enumerate(pred):  # detections per image\n",
    "                if webcam:  # batch_size >= 1\n",
    "                    p, s, frame = path[i], '%g: ' % i, dataset.count\n",
    "                else:\n",
    "                    p, s, frame = path, '', getattr(dataset, 'frame', 0)\n",
    "\n",
    "                # gn = torch.tensor(im0s.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "                if len(det):\n",
    "                    # Rescale boxes from img_size to im0 size\n",
    "                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n",
    "                    \n",
    "                                    # Print results\n",
    "                    for c in det[:, -1].unique():\n",
    "                        n = (det[:, -1] == c).sum()  # detections per class\n",
    "                        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                    for  *xyxy, conf, cls in reversed(det):\n",
    "                      label = f'{names[int(cls)]} {conf:.2f}'\n",
    "                      plot_one_box(xyxy, im0s, label=label, color=colors[int(cls)], line_thickness=1)\n",
    "                # Print time (inference + NMS)\n",
    "                print(f'{s}Done. ({(1E3 * (t2 - t1)):.1f}ms) Inference, ({(1E3 * (t3 - t2)):.1f}ms) NMS')\n",
    "                      \n",
    "                out.write(im0s)\n",
    "                # cv2_imshow(im0s)\n",
    "    vid_cap.release()\n",
    "    out.release()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b571d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행문\n",
    "with torch.no_grad():\n",
    "    if opt.update:  # update all models (to fix SourceChangeWarning)\n",
    "        for opt.weights in ['yolov7.pt']:\n",
    "            detect()\n",
    "            strip_optimizer(opt.weights)\n",
    "    else:\n",
    "        detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa9944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dc01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ede38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
